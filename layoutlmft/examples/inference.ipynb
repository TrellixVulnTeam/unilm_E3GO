{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minh/anaconda3/envs/cu11.1/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    PreTrainedTokenizerFast,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from layoutlmft.data.utils import load_image, normalize_bbox\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "from glob import glob\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/unilm/20220516_outputs/checkpoint-4000\"\n",
    "class_labels = ['O', 'B-OTHER', 'I-OTHER', 'B-SUPPLIER_NAME', 'I-SUPPLIER_NAME', 'B-SUPPLIER_ADDR', 'I-SUPPLIER_ADDR', 'B-TOTALAMOUNT', 'I-TOTALAMOUNT']\n",
    "label_to_id = {l: i for i, l in enumerate(class_labels)}\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=len(class_labels),\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    use_fast=True,\n",
    ")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_path,\n",
    "    from_tf=bool(\".ckpt\" in model_path),\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    padding = \"max_length\"\n",
    "    text_column_name = \"tokens\"\n",
    "    # label_column_name = \"ner_tags\"\n",
    "    # label_all_tokens = False\n",
    "    # label_to_id = {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8}\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "\n",
    "    # labels = []\n",
    "    bboxes = []\n",
    "    images = []\n",
    "    _word_ids = []\n",
    "\n",
    "    for batch_index in range(len(tokenized_inputs[\"input_ids\"])):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=batch_index)\n",
    "        _word_ids += word_ids\n",
    "        org_batch_index = tokenized_inputs[\"overflow_to_sample_mapping\"][batch_index]\n",
    "\n",
    "        # label = examples[label_column_name][org_batch_index]\n",
    "        bbox = examples[\"norm_bboxes\"][org_batch_index]\n",
    "        image = examples[\"image\"][org_batch_index]\n",
    "\n",
    "\n",
    "        previous_word_idx = None\n",
    "        # label_ids = []\n",
    "        bbox_inputs = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                # label_ids.append(-100)\n",
    "                bbox_inputs.append([0, 0, 0, 0])\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # label_ids.append(label_to_id[label[word_idx]])\n",
    "                bbox_inputs.append(bbox[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                # label_ids.append(label_to_id[label[word_idx]] if label_all_tokens else -100)\n",
    "                bbox_inputs.append(bbox[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        # labels.append(label_ids)\n",
    "        bboxes.append(bbox_inputs)\n",
    "        images.append(image)\n",
    "    # tokenized_inputs[\"labels\"] = labels\n",
    "    tokenized_inputs[\"bbox\"] = bboxes\n",
    "    tokenized_inputs[\"image\"] = images\n",
    "\n",
    "    overflow_mapping = tokenized_inputs[\"overflow_to_sample_mapping\"]\n",
    "    tokenized_inputs.pop(\"overflow_to_sample_mapping\", None)\n",
    "    \n",
    "    return tokenized_inputs, overflow_mapping, _word_ids\n",
    "\n",
    "def generate_example(image_path, json_path):\n",
    "    # ann_dir = os.path.join(filepath, \"annotations\")\n",
    "    # img_dir = os.path.join(filepath, \"images\")\n",
    "    tokens = []\n",
    "    bboxes = []\n",
    "    norm_bboxes = []\n",
    "    ner_tags = []\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf8\") as f:\n",
    "        data = json.load(f)\n",
    "    image, size = load_image(image_path)\n",
    "    for item in data[\"form\"]:\n",
    "        words, label = item[\"words\"], item[\"label\"]\n",
    "        words = [w for w in words if w[\"text\"].strip() != \"\"]\n",
    "        if len(words) == 0:\n",
    "            continue\n",
    "        if label == \"other\":\n",
    "            for w in words:\n",
    "                tokens.append(w[\"text\"])\n",
    "                ner_tags.append(\"O\")\n",
    "                bboxes.append(w[\"box\"])\n",
    "                norm_bboxes.append(normalize_bbox(w[\"box\"], size))\n",
    "        else:\n",
    "            tokens.append(words[0][\"text\"])\n",
    "            ner_tags.append(\"B-\" + label.upper())\n",
    "            bboxes.append(words[0][\"box\"])\n",
    "            norm_bboxes.append(normalize_bbox(words[0][\"box\"], size))\n",
    "            for w in words[1:]:\n",
    "                tokens.append(w[\"text\"])\n",
    "                ner_tags.append(\"I-\" + label.upper())\n",
    "                bboxes.append(w[\"box\"])\n",
    "                norm_bboxes.append(normalize_bbox(w[\"box\"], size))\n",
    "\n",
    "    return {\"tokens\": [tokens], \"bboxes\": [bboxes], \"norm_bboxes\": [norm_bboxes], \"ner_tags\": [ner_tags], \"image\": [image]}\n",
    "\n",
    "def convert_to_tensor(inputs):\n",
    "    inputs_t = dict()\n",
    "    for k, v in inputs.items():\n",
    "        if isinstance(v[0], list):\n",
    "            inputs_t[k] = torch.tensor(v)\n",
    "        elif isinstance(v[0], torch.Tensor):\n",
    "            inputs_t[k] = torch.stack(v)\n",
    "        else:\n",
    "            raise Exception(f\"{k} is a list of type {type(v[0])}\")\n",
    "    return inputs_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/invoice_data/testing_data/images/9508519816_page_1.jpg\"\n",
    "json_path = \"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/invoice_data/testing_data/annotations/9508519816_page_1.json\"\n",
    "# image_path = \"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/invoice_data/training_data/images/9533207517.jpg\"\n",
    "# json_path = \"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/invoice_data/training_data/annotations/9533207517.json\"\n",
    "# {'id': Value(dtype='string', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'bboxes': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-OTHER', 'I-OTHER', 'B-SUPPLIER_NAME', 'I-SUPPLIER_NAME', 'B-SUPPLIER_ADDR', 'I-SUPPLIER_ADDR', 'B-TOTALAMOUNT', 'I-TOTALAMOUNT'], id=None), length=-1, id=None), 'image': Array3D(shape=(3, 224, 224), dtype='uint8', id=None)}\n",
    "\n",
    "example = generate_example(image_path, json_path)\n",
    "# example\n",
    "inputs, overflow_mapping, word_ids = tokenize_and_align_labels(example)\n",
    "# inputs.pop(\"labels\", None)\n",
    "inputs_t = convert_to_tensor(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/minh/Storage/projects/EGS/InvoiceDataExtraction/unilm/layoutlmft/layoutlmft/models/layoutlmv2/modeling_layoutlmv2.py:723: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  // self.config.image_feature_pool_shape[1]\n",
      "/media/minh/Storage/projects/EGS/InvoiceDataExtraction/unilm/layoutlmft/layoutlmft/models/layoutlmv2/modeling_layoutlmv2.py:733: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  // self.config.image_feature_pool_shape[0]\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_dir = \"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/debug\"\n",
    "\n",
    "def debug(image_path, df):\n",
    "    img = cv2.imread(image_path)\n",
    "    for _, tag, _, class_name, word, box in df.itertuples(index=False):\n",
    "        x1, y1, x2, y2 = box\n",
    "        if class_name == \"TOTALAMOUNT\":\n",
    "            color = (0, 0, 255)\n",
    "        elif class_name == \"SUPPLIER_NAME\":\n",
    "            color = (255, 0, 0)\n",
    "        elif class_name == \"SUPPLIER_ADDR\":\n",
    "            color = (0, 255, 0)\n",
    "        img = cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "        img = cv2.putText(img, tag, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                   0.2, color, 2, cv2.LINE_AA)\n",
    "    output_path = os.path.join(debug_dir, os.path.basename(image_path))\n",
    "    cv2.imwrite(output_path, img)\n",
    "\n",
    "def same_box(b1, b2):\n",
    "    for x, y in zip(b1, b2):\n",
    "        if x != y:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def get_class(text):\n",
    "    l = text.split(\"-\")\n",
    "    return l[1] if len(l) == 2 else \"OTHER\"\n",
    "\n",
    "def refine(words, boxes, tags, word_ids):\n",
    "    df = pd.DataFrame({\"word_id\": word_ids, \"tag\": tags})\n",
    "    df.dropna(inplace=True)\n",
    "    df = df.astype({\"word_id\": \"int32\"})\n",
    "    df = df.drop_duplicates([\"word_id\"])\n",
    "    df = df[df[\"tag\"] != \"O\"]\n",
    "    df[\"prefix\"] = df[\"tag\"].map(lambda x : x.split(\"-\")[0])\n",
    "    df[\"class\"] = df[\"tag\"].map(get_class)\n",
    "    df[\"word\"] = df[\"word_id\"].map(lambda x : words[x])\n",
    "    df[\"bbox\"] = df[\"word_id\"].map(lambda x : boxes[x])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_boxes = inputs_t[\"bbox\"].reshape([-1, 4]).tolist()\n",
    "preds = torch.argmax(outputs.logits, -1).reshape([-1]).tolist()\n",
    "pred_tags = [class_labels[i] for i in preds]\n",
    "words = example[\"tokens\"][0]\n",
    "boxes = example[\"bboxes\"][0]\n",
    "# print(len(words), len(boxes), len(pred_tags), len(word_ids))\n",
    "df = refine(words, boxes, pred_tags, word_ids)\n",
    "debug(image_path, df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/minh/Storage/projects/EGS/InvoiceDataExtraction/unilm/layoutlmft/layoutlmft/models/layoutlmv2/modeling_layoutlmv2.py:723: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  // self.config.image_feature_pool_shape[1]\n",
      "/media/minh/Storage/projects/EGS/InvoiceDataExtraction/unilm/layoutlmft/layoutlmft/models/layoutlmv2/modeling_layoutlmv2.py:733: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  // self.config.image_feature_pool_shape[0]\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(debug_dir):\n",
    "    shutil.rmtree(debug_dir)\n",
    "os.makedirs(debug_dir)\n",
    "\n",
    "image_dir = \"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/20220516_invoice_data/testing_data/images\"\n",
    "json_dir = \"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/20220516_invoice_data/testing_data/annotations\"\n",
    "\n",
    "image_paths = glob(os.path.join(image_dir, \"*.jpg\"))\n",
    "for image_path in image_paths:\n",
    "    prefix = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    json_path = os.path.join(json_dir, f\"{prefix}.json\")\n",
    "\n",
    "\n",
    "    example = generate_example(image_path, json_path)\n",
    "    # example\n",
    "    inputs, overflow_mapping, word_ids = tokenize_and_align_labels(example)\n",
    "    # inputs.pop(\"labels\", None)\n",
    "    inputs_t = convert_to_tensor(inputs)\n",
    "    outputs = model(**inputs_t)\n",
    "    input_boxes = inputs_t[\"bbox\"].reshape([-1, 4]).tolist()\n",
    "    preds = torch.argmax(outputs.logits, -1).reshape([-1]).tolist()\n",
    "    pred_tags = [class_labels[i] for i in preds]\n",
    "    words = example[\"tokens\"][0]\n",
    "    boxes = example[\"bboxes\"][0]\n",
    "    # print(len(words), len(boxes), len(pred_tags), len(word_ids))\n",
    "    df = refine(words, boxes, pred_tags, word_ids)\n",
    "    debug(image_path, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "942140736990c62f967dadc2617fa6d17aee4a6bcb2f21bd66eacc795d470755"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('cu11.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
