{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYIl6EXVlEDt"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from layoutlmft.data.utils import load_image, normalize_bbox\n",
    "from detectron2.data.transforms import ResizeTransform, TransformList\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "from glob import glob\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pdf2image import convert_from_path\n",
    "from collections import Counter\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMHaHQy0lED2"
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GW0rwiLYlED5",
    "outputId": "22d0aabb-daa7-4f0c-e874-c18e943000c6"
   },
   "outputs": [],
   "source": [
    "model_path = \"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/unilm/20220516_outputs/checkpoint-4000\"\n",
    "class_labels = ['O', 'B-OTHER', 'I-OTHER', 'B-SUPPLIER_NAME', 'I-SUPPLIER_NAME', 'B-SUPPLIER_ADDR', 'I-SUPPLIER_ADDR', 'B-TOTALAMOUNT', 'I-TOTALAMOUNT']\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=len(class_labels),\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    use_fast=True,\n",
    ")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_path,\n",
    "    from_tf=bool(\".ckpt\" in model_path),\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8y7PCzQlEEB"
   },
   "source": [
    "## Create TextractReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JaO6MsglEEB",
    "outputId": "70afa58f-01b7-4b0f-d39d-231a7fe60c1f"
   },
   "outputs": [],
   "source": [
    "class TextractReader:\n",
    "\n",
    "    def __init__(self, textract_path, doc_dir):\n",
    "        self.doc_dir = doc_dir\n",
    "        self.textract_data = {}\n",
    "        self.parse_textract(textract_path)\n",
    "\n",
    "    def parse_textract(self, textract_path):\n",
    "        textract_data = pd.read_json(textract_path)\n",
    "        \n",
    "        for filename, res in textract_data.itertuples(index=False):\n",
    "            \n",
    "            # if filename not in [\"9508645819.pdf\"]: continue\n",
    "            if res.get(\"JobStatus\") not in [\"SUCCEEDED\", None]: \n",
    "                print(f\"SKIP {filename} due to {res.get('JobStatus')} status\")\n",
    "                continue\n",
    "            doc_path = os.path.join(self.doc_dir, filename)\n",
    "            if not os.path.exists(doc_path):\n",
    "                print(f\"SKIP {filename} because {doc_path} does not exist\")\n",
    "\n",
    "            page_num = res[\"DocumentMetadata\"][\"Pages\"]\n",
    "            blocks = self.parse_blocks(res[\"Blocks\"], page_num)\n",
    "            imgs = self.read_images(filename)\n",
    "\n",
    "            pages = {}\n",
    "            for i in range(page_num):\n",
    "                page_id = i+1\n",
    "                page_df = blocks[blocks[\"page\"] == page_id].copy()\n",
    "                page_df = page_df.reset_index(drop=True)\n",
    "\n",
    "                if len(page_df) == 0: \n",
    "                    continue\n",
    "                height, width, _ = imgs[i].shape\n",
    "                page_df[[\"x0\", \"x2\", \"xtl\", \"xbr\"]] *= width\n",
    "                page_df[[\"y0\", \"y2\", \"ytl\", \"ybr\"]] *= height\n",
    "                angle = self.check_orientation(page_df)\n",
    "                page_df = self.refine_orientation(page_df, angle, width, height)\n",
    "                page_df[\"page\"] = page_id\n",
    "                page_df = page_df[[\"xtl\", \"ytl\", \"xbr\", \"ybr\", \"text\", \"page\"]]\n",
    "                page_data = {\"ocr\": page_df, \"angle\": angle, \"width\": width, \"height\": height}\n",
    "                pages[page_id] = page_data\n",
    "\n",
    "            self.textract_data[filename] = pages\n",
    "            \n",
    "    def get_data(self, filename, return_img=True):\n",
    "        if self.textract_data.get(filename) is None:\n",
    "            return None\n",
    "            \n",
    "        pages = self.textract_data.get(filename)\n",
    "        if return_img:\n",
    "            imgs = self.read_images(filename)\n",
    "            for page_id, page_data in pages.items():\n",
    "                img = imgs[page_id - 1]\n",
    "                angle = page_data[\"angle\"]\n",
    "                angle_map = {90: cv2.ROTATE_90_CLOCKWISE, 180: cv2.ROTATE_180, 270: cv2.ROTATE_90_COUNTERCLOCKWISE}\n",
    "                if angle != 0:\n",
    "                    img = cv2.rotate(img, angle_map[angle])\n",
    "                pages[page_id][\"img\"] = img\n",
    "        return pages\n",
    "\n",
    "    \n",
    "    def read_images(self, filename):\n",
    "        doc_path = os.path.join(self.doc_dir, filename)\n",
    "        imgs = []\n",
    "        if filename[-3:] in [\"jpg\", \"png\"]:\n",
    "            img = cv2.imread(doc_path)\n",
    "            imgs.append(img)\n",
    "        elif filename[-3:] in [\"pdf\"]:\n",
    "            pil_imgs = convert_from_path(doc_path)\n",
    "            for i, img in enumerate(pil_imgs):\n",
    "                img = np.array(img)\n",
    "                img = img[:, :, ::-1]\n",
    "                imgs.append(img)\n",
    "        return imgs\n",
    "            \n",
    "    def parse_blocks(self, blocks, page_num):\n",
    "        block_df = pd.DataFrame()\n",
    "        for block in blocks:\n",
    "            if block[\"BlockType\"] != \"WORD\": continue\n",
    "            conf = block[\"Confidence\"]\n",
    "            text = block[\"Text\"]\n",
    "            page = block.get(\"Page\")\n",
    "            if page is None and page_num > 1:\n",
    "                raise Exception(\"Page is None while number of pages > 1\")\n",
    "            if page is None:\n",
    "                page = 1\n",
    "            polygon = block[\"Geometry\"][\"Polygon\"]\n",
    "            X = [p['X'] for p in polygon]\n",
    "            Y = [p['Y'] for p in polygon]\n",
    "            data = {\"x0\": X[0], \"y0\": Y[0], \"x2\": X[2], \"y2\": Y[2],\n",
    "                    \"xtl\": min(X), \"ytl\": min(Y), \"xbr\": max(X), \"ybr\": max(Y),\n",
    "                    \"text\": text, \"score\": conf, \"page\": page}\n",
    "            block_df = block_df.append(data, ignore_index=True)\n",
    "        return block_df\n",
    "\n",
    "    def check_orientation(self, df, ref_num=10):\n",
    "        df = df.copy()\n",
    "        df = df.sort_values(by=[\"score\"], ascending=False)\n",
    "        df = df[[\"x0\", \"y0\", \"x2\", \"y2\"]]\n",
    "        angles = []\n",
    "        for x0, y0, x2, y2 in df.head(ref_num).itertuples(index=False):\n",
    "            if x0 < x2 and y0 < y2:\n",
    "                angles.append(0)\n",
    "            elif x0 < x2 and y0 > y2:\n",
    "                angles.append(90)\n",
    "            elif x0 > x2 and y0 < y2:\n",
    "                angles.append(270)\n",
    "            else:\n",
    "                angles.append(180)\n",
    "        cnt = Counter(angles)\n",
    "        return cnt.most_common()[0][0]\n",
    "\n",
    "    def refine_orientation(self, textract_df, angle, width, height):\n",
    "        if angle == 90:\n",
    "            textract_df[\"xtl\"] = height - textract_df[\"y0\"]\n",
    "            textract_df[\"xbr\"] = height - textract_df[\"y2\"]\n",
    "            textract_df[\"ytl\"] = textract_df[\"x0\"]\n",
    "            textract_df[\"ybr\"] = textract_df[\"x2\"]\n",
    "        elif angle == 270:\n",
    "            textract_df[\"xtl\"] = textract_df[\"y0\"]\n",
    "            textract_df[\"xbr\"] = textract_df[\"y2\"]\n",
    "            textract_df[\"ytl\"] = width - textract_df[\"x0\"]\n",
    "            textract_df[\"ybr\"] = width - textract_df[\"x2\"]\n",
    "        elif angle == 180:\n",
    "            textract_df[\"xtl\"] = width - textract_df[\"x0\"]\n",
    "            textract_df[\"xbr\"] = width - textract_df[\"x2\"]\n",
    "            textract_df[\"ytl\"] = height - textract_df[\"y0\"]\n",
    "            textract_df[\"ybr\"] = height - textract_df[\"y2\"]\n",
    "        return textract_df\n",
    "\n",
    "textract_path = \"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/sdmgr_inference/textract_extraction_filtered.json\"\n",
    "doc_dir = \"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/sdmgr_inference/20220425_FilteredData\"\n",
    "textract_reader = TextractReader(textract_path, doc_dir)\n",
    "print(len(textract_reader.textract_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvjletHUlEER"
   },
   "source": [
    "## Layoutlmv2 Data Pipeline (SDMGR only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Hy4qrIklEES"
   },
   "outputs": [],
   "source": [
    "debug_dir = \"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/debug\"\n",
    "\n",
    "def debug(img, df, output_path):\n",
    "    img = img.copy()\n",
    "    for _, tag, _, class_name, word, box in df.itertuples(index=False):\n",
    "        x1, y1, x2, y2 = map(round, box)\n",
    "        if class_name == \"TOTALAMOUNT\":\n",
    "            color = (0, 0, 255)\n",
    "        elif class_name == \"SUPPLIER_NAME\":\n",
    "            color = (255, 0, 0)\n",
    "        elif class_name == \"SUPPLIER_ADDR\":\n",
    "            color = (0, 255, 0)\n",
    "        img = cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "        # img = cv2.putText(img, tag, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "        #            0.2, color, 2, cv2.LINE_AA)\n",
    "    cv2.imwrite(output_path, img)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    padding = \"max_length\"\n",
    "    text_column_name = \"tokens\"\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "\n",
    "    bboxes = []\n",
    "    images = []\n",
    "    _word_ids = []\n",
    "\n",
    "    for batch_index in range(len(tokenized_inputs[\"input_ids\"])):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=batch_index)\n",
    "        _word_ids += word_ids\n",
    "        org_batch_index = tokenized_inputs[\"overflow_to_sample_mapping\"][batch_index]\n",
    "\n",
    "        bbox = examples[\"norm_bboxes\"][org_batch_index]\n",
    "        image = examples[\"image\"][org_batch_index]\n",
    "\n",
    "\n",
    "        previous_word_idx = None\n",
    "        # label_ids = []\n",
    "        bbox_inputs = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                bbox_inputs.append([0, 0, 0, 0])\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                bbox_inputs.append(bbox[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                bbox_inputs.append(bbox[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        bboxes.append(bbox_inputs)\n",
    "        images.append(image)\n",
    "    tokenized_inputs[\"bbox\"] = bboxes\n",
    "    tokenized_inputs[\"image\"] = images\n",
    "\n",
    "    overflow_mapping = tokenized_inputs[\"overflow_to_sample_mapping\"]\n",
    "    tokenized_inputs.pop(\"overflow_to_sample_mapping\", None)\n",
    "    \n",
    "    return tokenized_inputs, overflow_mapping, _word_ids\n",
    "\n",
    "def transform_image(image):\n",
    "    h = image.shape[0]\n",
    "    w = image.shape[1]\n",
    "    img_trans = TransformList([ResizeTransform(h=h, w=w, new_h=224, new_w=224)])\n",
    "    image = torch.tensor(img_trans.apply_image(image).copy()).permute(2, 0, 1)  # copy to make it writeable\n",
    "    return image, (w, h)\n",
    "\n",
    "def generate_example(img, page_df):\n",
    "    tokens = []\n",
    "    bboxes = []\n",
    "    norm_bboxes = []\n",
    "    img, size = transform_image(img)\n",
    "    for xtl, ytl, xbr, ybr, text, _ in page_df.itertuples(index=False):\n",
    "        box = [xtl, ytl, xbr, ybr]\n",
    "        tokens.append(text)\n",
    "        bboxes.append(box)\n",
    "        norm_bboxes.append(normalize_bbox(box, size))\n",
    "\n",
    "    return {\"tokens\": [tokens], \"bboxes\": [bboxes], \"norm_bboxes\": [norm_bboxes], \"image\": [img]}\n",
    "\n",
    "def convert_to_tensor(inputs):\n",
    "    inputs_t = dict()\n",
    "    for k, v in inputs.items():\n",
    "        if isinstance(v[0], list):\n",
    "            inputs_t[k] = torch.tensor(v)\n",
    "        elif isinstance(v[0], torch.Tensor):\n",
    "            inputs_t[k] = torch.stack(v)\n",
    "        else:\n",
    "            raise Exception(f\"{k} is a list of type {type(v[0])}\")\n",
    "    return inputs_t\n",
    "\n",
    "def get_class(text):\n",
    "    l = text.split(\"-\")\n",
    "    return l[1] if len(l) == 2 else \"OTHER\"\n",
    "\n",
    "def refine(words, boxes, tags, scores, word_ids):\n",
    "    df = pd.DataFrame({\"word_id\": word_ids, \"tag\": tags, \"conf\": scores})\n",
    "    df.dropna(inplace=True)\n",
    "    df = df.astype({\"word_id\": \"int32\"})\n",
    "    df = df.drop_duplicates([\"word_id\"])\n",
    "    df = df[df[\"tag\"] != \"O\"]\n",
    "    df[\"prefix\"] = df[\"tag\"].map(lambda x : x.split(\"-\")[0])\n",
    "    df[\"class\"] = df[\"tag\"].map(get_class)\n",
    "    df[\"word\"] = df[\"word_id\"].map(lambda x : words[x])\n",
    "    df[\"bbox\"] = df[\"word_id\"].map(lambda x : boxes[x])\n",
    "    return df\n",
    "\n",
    "def infer_one_page(page_data):\n",
    "    res = pd.DataFrame(columns=[\"xtl\", \"ytl\", \"xbr\", \"ybr\", \"text\", \"label\", \"page\"])\n",
    "    class_map = {\"SUPPLIER_NAME\": \"supplier_name\", \"SUPPLIER_ADDR\": \"supplier_addr\", \"TOTALAMOUNT\": \"totalAmount\"}\n",
    "\n",
    "    img = page_data[\"img\"]\n",
    "    page_df = page_data[\"ocr\"]\n",
    "    page_id = np.unique(page_df[\"page\"]).item()\n",
    "\n",
    "    example = generate_example(img, page_df)\n",
    "    inputs, overflow_mapping, word_ids = tokenize_and_align_labels(example)\n",
    "    inputs_t = convert_to_tensor(inputs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs_t)\n",
    "        input_boxes = inputs_t[\"bbox\"].reshape([-1, 4]).tolist()\n",
    "        scores, preds = torch.max(outputs.logits, -1)\n",
    "        scores = scores.reshape([-1]).tolist()\n",
    "        preds = preds.reshape([-1]).tolist()\n",
    "    \n",
    "    pred_tags = [class_labels[i] for i in preds]\n",
    "    words = example[\"tokens\"][0]\n",
    "    boxes = example[\"bboxes\"][0]\n",
    "    df = refine(words, boxes, pred_tags, scores, word_ids)\n",
    "    # debug(img, df, os.path.join(debug_dir, \"commercial-invoice.jpg\"))\n",
    "    for _, tag, conf, _, class_name, word, box in df.itertuples(index=False):\n",
    "        x1, y1, x2, y2 = map(round, box)\n",
    "        res = res.append({\"xtl\": x1, \"ytl\": y1, \"xbr\": x2, \"ybr\": y2, \"text\": word, \"label\": class_map[class_name], \"conf\": conf, \"page\": page_id}, ignore_index=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwNYQ07klEEY"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhbLDdUnlEEZ",
    "outputId": "2340c123-7e0f-4ca5-d508-80377c599c0c"
   },
   "outputs": [],
   "source": [
    "# if os.path.isdir(debug_dir):\n",
    "#     shutil.rmtree(debug_dir)\n",
    "# os.makedirs(debug_dir)\n",
    "\n",
    "def infer_one_doc(doc_data):\n",
    "    doc_df = pd.DataFrame(columns=[\"xtl\", \"ytl\", \"xbr\", \"ybr\", \"text\", \"label\", \"conf\", \"page\"])\n",
    "    for page_id, page_data in doc_data.items():\n",
    "        page_df = infer_one_page(page_data)\n",
    "        doc_df = doc_df.append(page_df, ignore_index=True)\n",
    "    return doc_df\n",
    "\n",
    "# input_dir = \"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/test\"\n",
    "input_dir = \"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/sdmgr_inference/20220425_FilteredData\"\n",
    "\n",
    "output_dir = \"20220530_layoutlmv2_outputs\"\n",
    "if os.path.isdir(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir)\n",
    "fnames = [\"9498101535.jpg\"]\n",
    "filenames = [x for x in os.listdir(input_dir) if os.path.splitext(x)[1] in [\".jpg\", \".png\", \".pdf\"]]\n",
    "\n",
    "for filename in filenames:\n",
    "    # if filename not in fnames: continue\n",
    "    prefix = os.path.splitext(filename)[0]\n",
    "    doc_data = textract_reader.get_data(filename)\n",
    "    if doc_data is None:\n",
    "        print(f\"Skip {filename} because Textract data is missing\")\n",
    "        continue\n",
    "    doc_df = infer_one_doc(doc_data)\n",
    "    # optional debug\n",
    "    doc_df.to_csv(os.path.join(output_dir, f\"{prefix}.csv\"), sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRhrqLhFlEEb"
   },
   "source": [
    "## Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExtractor:\n",
    "    def __init__(self, country_currency_map_file, symbol_currency_map_file, textract_data):\n",
    "        self.country_code = pd.read_csv(country_currency_map_file)\n",
    "        self.country_code[\"CountryCode\"] = self.country_code[\"CountryCode\"].astype(\"str\")\n",
    "        self.money_symbol = pd.read_csv(symbol_currency_map_file)\n",
    "        self.textract_data = textract_data\n",
    "        \n",
    "    def __call__(self, doc_kie_df, doc_ocr_data):\n",
    "        res = {\"supplier_name\": \"\", \"totalAmount\": \"\", \"currencyCode\": \"\"}\n",
    "        doc_kie_df = self.merge_words(doc_kie_df)\n",
    "        total_amount, total_amount_page, total_amount_full_text, total_amount_box = self.extract_total_amount(doc_kie_df)\n",
    "        res['supplier_name'] = self.extract_supplier_name(doc_kie_df, total_amount_page)\n",
    "        res['totalAmount'] = total_amount\n",
    "        address = self.extract_supplier_address(doc_kie_df, total_amount_page)\n",
    "        res['currencyCode'] = self.extract_currency_all_page(doc_kie_df, doc_ocr_data, total_amount, total_amount_page, total_amount_full_text, total_amount_box, address)\n",
    "        return res\n",
    "        \n",
    "    def process_money_text(self, text):\n",
    "        m = re.search(\"\\d+(?:.\\d|,\\d)*\\d*\", text)\n",
    "        if m is not None:\n",
    "            subtext = m.group()\n",
    "            subtext = re.sub(\",\", \".\", subtext)\n",
    "            subtext = re.sub(\"[.](?=.*[.]|\\d{3})\", \"\", subtext)\n",
    "            try:\n",
    "                t = float(subtext)\n",
    "            except:\n",
    "                pass\n",
    "            else:\n",
    "                return str(float(subtext))\n",
    "        return None\n",
    "\n",
    "    def extract_total_amount(self, doc_kie_df):\n",
    "        total_amount = \"\"\n",
    "        total_amount_full_text = \"\"\n",
    "        total_amount_box = None\n",
    "        total_amount_page = None\n",
    "\n",
    "        for _, page_kie_df in doc_kie_df.groupby(\"page\"):\n",
    "            max_score = 0\n",
    "            for text, _, conf, xtl, ytl, xbr, ybr, page_id in page_kie_df[page_kie_df[\"label\"] == \"totalAmount\"].itertuples(index=False):\n",
    "                subtext = self.process_money_text(text)\n",
    "                if subtext is not None and conf > max_score:\n",
    "                    total_amount = subtext\n",
    "                    total_amount_full_text = text\n",
    "                    total_amount_box = [xtl, ytl, xbr, ybr]\n",
    "                    total_amount_page = page_id\n",
    "            if total_amount:\n",
    "                break\n",
    "        \n",
    "        return total_amount, total_amount_page, total_amount_full_text, total_amount_box\n",
    "\n",
    "    def extract_supplier_name(self, doc_kie_df, total_amount_page):\n",
    "        supplier_name = \"\"\n",
    "        name_pool = doc_kie_df[(doc_kie_df[\"page\"] == total_amount_page) & (doc_kie_df[\"label\"] == \"supplier_name\")][\"text\"].tolist()\n",
    "        if name_pool:\n",
    "            supplier_name = sorted(name_pool, key=lambda x : len(x.split()), reverse=True)[0]\n",
    "        if not supplier_name:\n",
    "            for page, post_page_df in doc_kie_df.groupby(\"page\"):\n",
    "                name_pool = post_page_df[post_page_df[\"label\"] == \"supplier_name\"][\"text\"].tolist()\n",
    "                if name_pool:\n",
    "                    supplier_name = sorted(name_pool, key=lambda x : len(x.split()), reverse=True)[0]\n",
    "                    break\n",
    "        if \"Google\".lower() in supplier_name.lower():\n",
    "            supplier_name = \"Google\"\n",
    "        elif \"Microsoft\".lower() in supplier_name.lower():\n",
    "            supplier_name = \"Microsoft\"\n",
    "        elif \"Grab\".lower() in supplier_name.lower():\n",
    "            supplier_name = \"Grab\"\n",
    "        return supplier_name\n",
    "\n",
    "    def extract_supplier_address(self, post_df, total_amount_page):\n",
    "        addresses = post_df[(post_df[\"label\"] == 'supplier_addr') & (post_df[\"page\"] == total_amount_page)][\"text\"].tolist()\n",
    "        address = \" \".join(addresses)\n",
    "        return address\n",
    "\n",
    "    def open_json_file(self, filename, textract_data):\n",
    "        file_name=[filename[:10]+'.jpg',filename[:10]+'.pdf']\n",
    "        if file_name[0] in textract_data.keys():\n",
    "            for i in textract_data[file_name[0]]['pages']:\n",
    "                if file_name[0][:-4]+'_page_'+str(i['page_id'])==filename:\n",
    "                    return i['data'], i['width'], i['height']\n",
    "        if file_name[1] in textract_data.keys():\n",
    "            for i in textract_data[file_name[1]]['pages']:\n",
    "                if file_name[0][:-4]+'_page_'+str(i['page_id'])==filename:\n",
    "                    return i['data'], i['width'], i['height']\n",
    "                    \n",
    "    def extract_currency_one_page(self, doc_ocr_data, page, total_amount, total_amount_full_text, total_amount_box, address, currencyCode):\n",
    "        if currencyCode:\n",
    "            return currencyCode\n",
    "        check=False\n",
    "\n",
    "        page_ocr_data = doc_ocr_data[page]\n",
    "        if total_amount:\n",
    "            if 'd' in total_amount_full_text:\n",
    "                return 'VND'\n",
    "            for symbol, Code in self.money_symbol.itertuples(index=False):\n",
    "                if symbol in total_amount_full_text:\n",
    "                    if symbol == '$':\n",
    "                        check = True\n",
    "                    else:\n",
    "                        return Code\n",
    "\n",
    "            bbox = total_amount_box\n",
    "            \n",
    "            bw = bbox[2] - bbox[0]\n",
    "            bh = bbox[3] - bbox[1]\n",
    "            x0 = bbox[0] - bw\n",
    "            y0 = bbox[1] - 4*bh\n",
    "            x2 = bbox[2] + bw\n",
    "            y2 = bbox[3] + 4*bh\n",
    "            \n",
    "            \n",
    "            list_currency_outside=[]\n",
    "            list_currency_inside=[]\n",
    "            \n",
    "            for xtl, ytl, xbr, ybr, text, page in page_ocr_data[\"ocr\"].itertuples(index=False):\n",
    "                check_box = [xtl, ytl, xbr, ybr]\n",
    "                \n",
    "                for symbol, currencyCode in self.money_symbol.itertuples(index=False):\n",
    "                    pattern = ['[a-zA-Z]+' + symbol, symbol + '[a-zA-Z]+']\n",
    "                    if symbol not in text: continue\n",
    "                    elif re.search(pattern[0], text) or re.search(pattern[1], text):\n",
    "                        continue\n",
    "                    else:\n",
    "                        if symbol == '$':\n",
    "                            check = True\n",
    "                            \n",
    "                        if check_box[0] >= x0 and check_box[2] <= x2:\n",
    "                            list_currency_inside.append(currencyCode)\n",
    "                        elif check_box[1] >= y0 and check_box[3] <= y2:\n",
    "                            list_currency_inside.append(currencyCode)\n",
    "                        else:\n",
    "                            list_currency_outside.append(currencyCode)\n",
    "\n",
    "            if check: \n",
    "                if 'SGD' in list_currency_inside or 'SGD' in list_currency_outside or 'Singapore'.lower() in address.lower():    \n",
    "                    return 'SGD'\n",
    "            if list_currency_inside:\n",
    "                if check:\n",
    "                    if len(set(list_currency_inside + list_currency_outside)) == 2 and '$' in list_currency_inside:\n",
    "                        list_currency = set(list_currency_inside+list_currency_outside)\n",
    "                        list_currency.remove('$')\n",
    "                        return list_currency.pop()      \n",
    "                return self.most_frequent(list_currency_inside)\n",
    "            elif list_currency_outside:\n",
    "                return self.most_frequent(list_currency_outside)\n",
    "        else:\n",
    "            list_currency_all=[]\n",
    "            for xtl, ytl, xbr, ybr, text, page in page_ocr_data[\"ocr\"].itertuples(index=False):\n",
    "                for symbol, currencyCode in self.money_symbol.itertuples(index=False):\n",
    "                    pattern=['[a-zA-Z]+' + symbol, symbol + '[a-zA-Z]+']\n",
    "                    if symbol not in text: continue\n",
    "                    elif re.search(pattern[0], text) or re.search(pattern[1], text):\n",
    "                        continue\n",
    "                    else:\n",
    "                        if symbol == '$':\n",
    "                            check = True\n",
    "                        list_currency_all.append(currencyCode)\n",
    "            if list_currency_all:\n",
    "                if check: \n",
    "                    if 'SGD' in list_currency_all or 'Singapore'.lower() in address.lower():    \n",
    "                        return 'SGD'\n",
    "                    if len(set(list_currency_all)) == 2 and '$' in list_currency_all:\n",
    "                        list_currency = set(list_currency_all)\n",
    "                        list_currency.remove('$')\n",
    "                        return list_currency.pop()\n",
    "            return self.most_frequent(list_currency_all)            \n",
    "        if address:\n",
    "            for index in range(len(self.country_code)):\n",
    "                if str(self.country_code.loc[index].Country) in address or str(self.country_code.loc[index].CountryCode) in address:\n",
    "                    return self.country_code.loc[index].Code\n",
    "        return \"\"\n",
    "\n",
    "    def extract_currency_all_page(self, doc_kie_df, doc_ocr_data, total_amount, total_amount_page, total_amount_full_text, total_amount_box, address):\n",
    "        currency = \"\"\n",
    "        all_pages = set()\n",
    "        if total_amount:\n",
    "            currency = self.extract_currency_one_page(doc_ocr_data, total_amount_page, total_amount, total_amount_full_text, total_amount_box, address, currency)\n",
    "            all_pages.add(total_amount_page)\n",
    "            \n",
    "        if not currency:\n",
    "            for text, label, conf, xtl, ytl, xbr, ybr, page in doc_kie_df.itertuples(index=False):\n",
    "                if page in all_pages:\n",
    "                    continue\n",
    "                if total_amount != \"\": \n",
    "                    if label == 'totalAmount' and total_amount == self.process_money_text(text):\n",
    "                        bbox = [xtl, ytl, xbr, ybr]\n",
    "                        currency = self.extract_currency_one_page(doc_ocr_data, page, total_amount, text, bbox, address, currency)\n",
    "                        all_pages.add(page)\n",
    "                        if currency != \"\":\n",
    "                            break\n",
    "\n",
    "        if not currency:\n",
    "            for text, label, conf, xtl, ytl, xbr, ybr, page in doc_kie_df.itertuples(index=False):\n",
    "                if page in all_pages:\n",
    "                    continue\n",
    "                currency = self.extract_currency_one_page(doc_ocr_data, page, \"\", \"\", None, address, currency)\n",
    "                all_pages.add(page)\n",
    "                if currency != \"\":\n",
    "                    break\n",
    "        if currency == \"$\":\n",
    "            currency = \"USD\"\n",
    "        return currency\n",
    "    \n",
    "    def most_frequent(self, l):\n",
    "        if l:\n",
    "            return Counter(l).most_common()[0][0]\n",
    "        return None\n",
    "\n",
    "    def is_adjacent(self, bi, bj):\n",
    "        if bi[\"label\"] != bj[\"label\"] or bi[\"page\"] != bj[\"page\"]:\n",
    "            return False\n",
    "        min_h = min(bi[\"ybr\"] - bi[\"ytl\"], bj[\"ybr\"] - bj[\"ytl\"])\n",
    "        min_w = min(bi[\"xbr\"] - bi[\"xtl\"], bj[\"xbr\"] - bj[\"xtl\"])\n",
    "        dx = min(bi[\"xbr\"], bj[\"xbr\"]) - max(bi[\"xtl\"], bj[\"xtl\"])\n",
    "        dy = min(bi[\"ybr\"], bj[\"ybr\"]) - max(bi[\"ytl\"], bj[\"ytl\"])\n",
    "\n",
    "        dxr = dx/min_w\n",
    "        dyr = dy/min_h\n",
    "\n",
    "        if bi[\"label\"] in [\"supplier_name\", \"supplier_addr\"] and dxr > 0.1 and dy > -min_h/2:\n",
    "            return True\n",
    "        if dyr > 0.2 and dx > -min_h:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def dfs(self, adj_mat, i, groups, gid):\n",
    "        if groups[i] > -1:\n",
    "            return groups\n",
    "        groups[i] = gid\n",
    "        for j in range(len(adj_mat[i])):\n",
    "            if adj_mat[i, j] == 1:\n",
    "                groups = self.dfs(adj_mat, j, groups, gid)\n",
    "        return groups\n",
    "\n",
    "    def merge_words(self, df):\n",
    "        res = pd.DataFrame(columns=[\"text\", \"label\", \"conf\", \"xtl\", \"ytl\", \"xbr\", \"ybr\", \"page\"])\n",
    "        if len(df) == 0:\n",
    "            return res\n",
    "        df = df.reset_index(drop=True)\n",
    "        adj_mat = np.zeros((len(df), len(df)), dtype=np.int32)\n",
    "        for i, bi in df.iterrows():\n",
    "            for j, bj in df.iterrows():\n",
    "                if self.is_adjacent(bi, bj):\n",
    "                    adj_mat[i][j] = 1\n",
    "        \n",
    "        groups = [-1] * len(adj_mat)\n",
    "        gid = 0\n",
    "        for i in range(len(adj_mat)):\n",
    "            groups = self.dfs(adj_mat, i, groups, gid)\n",
    "            gid += 1\n",
    "        df[\"group\"] = groups\n",
    "        for _, group_df in df.groupby(\"group\"):\n",
    "            label = np.unique(group_df[\"label\"]).item()\n",
    "            text = \" \".join(group_df[\"text\"].tolist())\n",
    "            conf = group_df[\"conf\"].mean()\n",
    "            xtl = group_df[\"xtl\"].min()\n",
    "            ytl = group_df[\"ytl\"].min()\n",
    "            xbr = group_df[\"xbr\"].max()\n",
    "            ybr = group_df[\"ybr\"].max()\n",
    "            page_id = np.unique(group_df[\"page\"]).item()\n",
    "            res = res.append({\"text\": text, \"label\": label, \"conf\": conf, \"xtl\": xtl, \"ytl\": ytl, \"xbr\": xbr, \"ybr\": ybr, \"page\": page_id}, ignore_index=True)\n",
    "        return res\n",
    "\n",
    "        \n",
    "extractor = DataExtractor(\"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/sdmgr_inference/country_currency_mapping.csv\", \"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/sdmgr_inference/currency_symbol_mapping_2.csv\", textract_reader.textract_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_HFQHhtlEEe",
    "outputId": "06fe4e2f-1492-4b2e-859c-d24c0b881c4b"
   },
   "outputs": [],
   "source": [
    "result_file = \"20220530_layoutlmv2_results.csv\"\n",
    "csv_dir = \"20220530_layoutlmv2_outputs\"\n",
    "filenames = os.listdir(doc_dir)\n",
    "\n",
    "# fnames = [\"9508645819\"]\n",
    "out_df = pd.DataFrame(columns=[\"code\", \"supplier_name\", \"totalAmount\", \"currencyCode\"])\n",
    "\n",
    "for filename in filenames:\n",
    "    prefix = os.path.splitext(filename)[0]\n",
    "    # if prefix not in fnames: continue\n",
    "    csv_path = os.path.join(csv_dir, f\"{prefix}.csv\")\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"Skip {filename} because {csv_path} does not exists\")\n",
    "        continue\n",
    "    print(filename)\n",
    "    doc_ocr_data = textract_reader.get_data(filename, return_img=False)\n",
    "    doc_kie_df = pd.read_csv(csv_path)\n",
    "    doc_kie_df = doc_kie_df.astype({\"text\": \"str\"})\n",
    "    doc_out = {\"code\": prefix}\n",
    "    ext_out = extractor(doc_kie_df, doc_ocr_data)\n",
    "    doc_out.update(ext_out)\n",
    "    print(doc_out)\n",
    "    out_df = out_df.append(doc_out, ignore_index=True)\n",
    "\n",
    "out_df.to_csv(result_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"20220530_layoutlmv2_results.csv\"\n",
    "template_file = \"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/20220519_results_eval.xlsx\"\n",
    "df = pd.read_csv(result_file)\n",
    "tp = pd.read_excel(template_file)\n",
    "\n",
    "merged = tp.merge(df, on=\"code\", how=\"inner\")\n",
    "merged[\"pred_supplier_name\"] = merged[\"supplier_name\"]\n",
    "merged[\"pred_totalAmount\"] = merged[\"totalAmount\"]\n",
    "merged[\"pred_currencyCode\"] = merged[\"currencyCode\"]\n",
    "merged['Correct Amount (TRUE/FALSE)'] = \"\"\n",
    "merged['Correct Supplier (TRUE/FALSE)'] = \"\"\n",
    "merged['Correct Currency (TRUE/FALSE)'] = \"\"\n",
    "merged = merged[['code', 'pred_supplier_name', 'true_supplier_name', 'Supplier name present (TRUE/FALSE)', 'Correct Supplier (TRUE/FALSE)', 'pred_totalAmount', 'true_totalAmount', 'total amount present (TRUE/FALSE)', 'Correct Amount (TRUE/FALSE)', 'Autopub', 'pred_currencyCode', 'true_currencyCode', 'Correct Currency (TRUE/FALSE)', 'Autopublish']]\n",
    "merged.to_excel(\"20220530_layoutlmv2_results_eval.xlsx\", index=False)\n",
    "print(np.sum(merged[\"pred_totalAmount\"] == merged[\"true_totalAmount\"])/len(merged))\n",
    "print(np.sum(merged[\"pred_currencyCode\"] == merged[\"true_currencyCode\"])/len(merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "915 - np.sum(merged[\"pred_currencyCode\"] == merged[\"true_currencyCode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sdmgr_full_pipeline_ver2.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "79c409b19b0ca8158257d882acad3ac1951c236caedae88ab1772a2d37c99612"
  },
  "kernelspec": {
   "display_name": "layoutlmft",
   "language": "python",
   "name": "layoutlmft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
