{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from layoutlmft import AutoModelForTokenClassification\n",
    "from layoutlmft.data.utils import load_image, normalize_bbox\n",
    "from detectron2.data.transforms import ResizeTransform, TransformList\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "from glob import glob\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pdf2image import convert_from_path\n",
    "from collections import Counter\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LayoutLMv2ForTokenClassification:\n\tsize mismatch for classifier.weight: copying a param with shape torch.Size([7, 768]) from checkpoint, the shape in current model is torch.Size([11, 768]).\n\tsize mismatch for classifier.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([11]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2ab25710f49a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".ckpt\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/cu11.1/lib/python3.7/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         raise ValueError(\n\u001b[1;32m    448\u001b[0m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cu11.1/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2066\u001b[0m                 \u001b[0msharded_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msharded_metadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2067\u001b[0m                 \u001b[0m_fast_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fast_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2068\u001b[0;31m                 \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2069\u001b[0m             )\n\u001b[1;32m   2070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cu11.1/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m   2249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2250\u001b[0m             \u001b[0merror_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2251\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error(s) in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LayoutLMv2ForTokenClassification:\n\tsize mismatch for classifier.weight: copying a param with shape torch.Size([7, 768]) from checkpoint, the shape in current model is torch.Size([11, 768]).\n\tsize mismatch for classifier.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([11])."
     ]
    }
   ],
   "source": [
    "model_path = \"/home/minh/Storage/projects/EGS/InvoiceDataExtraction/unilm/20220516_outputs/checkpoint-4000\"\n",
    "class_labels = ['O', 'B-OTHER', 'I-OTHER', 'B-SUPPLIER_NAME', 'I-SUPPLIER_NAME', 'B-SUPPLIER_ADDR', 'I-SUPPLIER_ADDR', 'B-TOTALAMOUNT', 'I-TOTALAMOUNT']\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=len(class_labels),\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    use_fast=True,\n",
    ")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_path,\n",
    "    from_tf=bool(\".ckpt\" in model_path),\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# model_path = \"/home/minh/Storage/projects/EGS/VetDoc/20220713_models/checkpoint-1000\"\n",
    "# class_labels = [\"O\", \"B-OTHER\", \"B-PROVIDER\", \"B-DATE\", \"B-NUMBER\", \"B-RECIPIENT\", \"I-OTHER\", \"I-PROVIDER\", \"I-DATE\", \"I-NUMBER\", \"I-RECIPIENT\"]\n",
    "\n",
    "# config = AutoConfig.from_pretrained(\n",
    "#     model_path,\n",
    "#     num_labels=len(class_labels),\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     model_path,\n",
    "#     use_fast=True,\n",
    "# )\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\n",
    "#     model_path,\n",
    "#     from_tf=bool(\".ckpt\" in model_path),\n",
    "#     config=config\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextractReader:\n",
    "\n",
    "    def __init__(self, textract_path, doc_dir):\n",
    "        self.doc_dir = doc_dir\n",
    "        self.textract_data = {}\n",
    "        self.parse_textract(textract_path)\n",
    "\n",
    "    def parse_textract(self, textract_path):\n",
    "        textract_data = pd.read_json(textract_path, encoding=\"utf-8\")\n",
    "        for filename, res in textract_data.itertuples(index=False):\n",
    "            # if filename not in [\"9498101535.jpg\"]: continue\n",
    "            if res.get(\"JobStatus\") not in [\"SUCCEEDED\", None]: \n",
    "                print(f\"SKIP {filename} due to {res.get('JobStatus')} status\")\n",
    "                continue\n",
    "            doc_path = os.path.join(self.doc_dir, filename)\n",
    "            if not os.path.exists(doc_path):\n",
    "                print(f\"SKIP {filename} because {doc_path} does not exist\")\n",
    "\n",
    "            page_num = res[\"DocumentMetadata\"][\"Pages\"]\n",
    "            blocks = self.parse_blocks(res[\"Blocks\"], page_num)\n",
    "            imgs = self.read_images(filename)\n",
    "\n",
    "            pages = []\n",
    "            for i in range(page_num):\n",
    "                page_id = i+1\n",
    "                page_df = blocks[blocks[\"page\"] == page_id].copy()\n",
    "                page_df = page_df.reset_index(drop=True)\n",
    "\n",
    "                if len(page_df) == 0: \n",
    "                    continue\n",
    "                height, width, _ = imgs[i].shape\n",
    "                page_df[[\"x0\", \"x2\", \"xtl\", \"xbr\"]] *= width\n",
    "                page_df[[\"y0\", \"y2\", \"ytl\", \"ybr\"]] *= height\n",
    "                angle = self.check_orientation(page_df)\n",
    "                page_df = self.refine_orientation(page_df, angle, width, height)\n",
    "                page_df[\"page\"] = page_id\n",
    "                page_df = page_df[[\"xtl\", \"ytl\", \"xbr\", \"ybr\", \"text\", \"page\"]]\n",
    "                pages.append({\"data\": page_df, \"angle\": angle, \"page_id\": page_id})\n",
    "            doc_data = {\"pages\": pages, \"page_num\": page_num}\n",
    "            self.textract_data[filename] = doc_data\n",
    "\n",
    "    def get_data(self, filename, return_img=True):\n",
    "        if self.textract_data.get(filename) is None:\n",
    "            return None\n",
    "            \n",
    "        page_num = self.textract_data[filename][\"page_num\"]\n",
    "        if return_img:\n",
    "            imgs = self.read_images(filename)\n",
    "            assert len(imgs) == page_num\n",
    "        else:\n",
    "            imgs = []\n",
    "        pages = self.textract_data[filename][\"pages\"]\n",
    "\n",
    "        data = []\n",
    "        for i, page in enumerate(pages):\n",
    "            item = {}\n",
    "            page_df = page[\"data\"]\n",
    "            item[\"anno\"] = page_df\n",
    "            if return_img:\n",
    "                angle = page[\"angle\"]\n",
    "                img = imgs[page[\"page_id\"]-1]\n",
    "                angle_map = {90: cv2.ROTATE_90_CLOCKWISE, 180: cv2.ROTATE_180, 270: cv2.ROTATE_90_COUNTERCLOCKWISE}\n",
    "                if angle != 0:\n",
    "                    img = cv2.rotate(img, angle_map[angle])\n",
    "                item[\"img\"] = img\n",
    "            data.append(item)\n",
    "        return data\n",
    "\n",
    "    \n",
    "    def read_images(self, filename):\n",
    "        doc_path = os.path.join(self.doc_dir, filename)\n",
    "        imgs = []\n",
    "        if filename[-3:] in [\"jpg\", \"png\"]:\n",
    "            img = cv2.imread(doc_path)\n",
    "            imgs.append(img)\n",
    "        elif filename[-3:] in [\"pdf\"]:\n",
    "            pil_imgs = convert_from_path(doc_path)\n",
    "            for i, img in enumerate(pil_imgs):\n",
    "                img = np.array(img)\n",
    "                img = img[:, :, ::-1]\n",
    "                imgs.append(img)\n",
    "        return imgs\n",
    "            \n",
    "    def parse_blocks(self, blocks, page_num):\n",
    "        block_df = pd.DataFrame()\n",
    "        for block in blocks:\n",
    "            if block[\"BlockType\"] != \"WORD\": continue\n",
    "            conf = block[\"Confidence\"]\n",
    "            text = block[\"Text\"]\n",
    "            page = block.get(\"Page\")\n",
    "            if page is None and page_num > 1:\n",
    "                raise Exception(\"Page is None while number of pages > 1\")\n",
    "            if page is None:\n",
    "                page = 1\n",
    "            polygon = block[\"Geometry\"][\"Polygon\"]\n",
    "            X = [p['X'] for p in polygon]\n",
    "            Y = [p['Y'] for p in polygon]\n",
    "            data = {\"x0\": X[0], \"y0\": Y[0], \"x2\": X[2], \"y2\": Y[2],\n",
    "                    \"xtl\": min(X), \"ytl\": min(Y), \"xbr\": max(X), \"ybr\": max(Y),\n",
    "                    \"text\": text, \"score\": conf, \"page\": page}\n",
    "            block_df = block_df.append(data, ignore_index=True)\n",
    "        return block_df\n",
    "\n",
    "    def check_orientation(self, df, ref_num=10):\n",
    "        df = df.copy()\n",
    "        df = df.sort_values(by=[\"score\"], ascending=False)\n",
    "        df = df[[\"x0\", \"y0\", \"x2\", \"y2\"]]\n",
    "        angles = []\n",
    "        for x0, y0, x2, y2 in df.head(ref_num).itertuples(index=False):\n",
    "            if x0 < x2 and y0 < y2:\n",
    "                angles.append(0)\n",
    "            elif x0 < x2 and y0 > y2:\n",
    "                angles.append(90)\n",
    "            elif x0 > x2 and y0 < y2:\n",
    "                angles.append(270)\n",
    "            else:\n",
    "                angles.append(180)\n",
    "        cnt = Counter(angles)\n",
    "        return cnt.most_common()[0][0]\n",
    "\n",
    "    def refine_orientation(self, textract_df, angle, width, height):\n",
    "        if angle == 90:\n",
    "            textract_df[\"xtl\"] = height - textract_df[\"y0\"]\n",
    "            textract_df[\"xbr\"] = height - textract_df[\"y2\"]\n",
    "            textract_df[\"ytl\"] = textract_df[\"x0\"]\n",
    "            textract_df[\"ybr\"] = textract_df[\"x2\"]\n",
    "        elif angle == 270:\n",
    "            textract_df[\"xtl\"] = textract_df[\"y0\"]\n",
    "            textract_df[\"xbr\"] = textract_df[\"y2\"]\n",
    "            textract_df[\"ytl\"] = width - textract_df[\"x0\"]\n",
    "            textract_df[\"ybr\"] = width - textract_df[\"x2\"]\n",
    "        elif angle == 180:\n",
    "            textract_df[\"xtl\"] = width - textract_df[\"x0\"]\n",
    "            textract_df[\"xbr\"] = width - textract_df[\"x2\"]\n",
    "            textract_df[\"ytl\"] = height - textract_df[\"y0\"]\n",
    "            textract_df[\"ybr\"] = height - textract_df[\"y2\"]\n",
    "        return textract_df\n",
    "\n",
    "# textract_reader = TextractReader(\"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/test/commercial-invoice.json\", \"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/test\")\n",
    "textract_reader = TextractReader(\"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/sdmgr_inference/textract_extraction_filtered.json\", \"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/sdmgr_inference/20220425_FilteredData\")\n",
    "print(len(textract_reader.textract_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_dir = \"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/debug\"\n",
    "\n",
    "def debug(img, df, output_path):\n",
    "    img = img.copy()\n",
    "    for _, tag, _, class_name, word, box in df.itertuples(index=False):\n",
    "        x1, y1, x2, y2 = map(round, box)\n",
    "        if class_name == \"TOTALAMOUNT\":\n",
    "            color = (0, 0, 255)\n",
    "        elif class_name == \"SUPPLIER_NAME\":\n",
    "            color = (255, 0, 0)\n",
    "        elif class_name == \"SUPPLIER_ADDR\":\n",
    "            color = (0, 255, 0)\n",
    "        img = cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "        # img = cv2.putText(img, tag, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "        #            0.2, color, 2, cv2.LINE_AA)\n",
    "    cv2.imwrite(output_path, img)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    padding = \"max_length\"\n",
    "    text_column_name = \"tokens\"\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "\n",
    "    bboxes = []\n",
    "    images = []\n",
    "    _word_ids = []\n",
    "\n",
    "    for batch_index in range(len(tokenized_inputs[\"input_ids\"])):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=batch_index)\n",
    "        _word_ids += word_ids\n",
    "        org_batch_index = tokenized_inputs[\"overflow_to_sample_mapping\"][batch_index]\n",
    "\n",
    "        bbox = examples[\"norm_bboxes\"][org_batch_index]\n",
    "        image = examples[\"image\"][org_batch_index]\n",
    "\n",
    "\n",
    "        previous_word_idx = None\n",
    "        # label_ids = []\n",
    "        bbox_inputs = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                bbox_inputs.append([0, 0, 0, 0])\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                bbox_inputs.append(bbox[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                bbox_inputs.append(bbox[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        bboxes.append(bbox_inputs)\n",
    "        images.append(image)\n",
    "    tokenized_inputs[\"bbox\"] = bboxes\n",
    "    tokenized_inputs[\"image\"] = images\n",
    "\n",
    "    overflow_mapping = tokenized_inputs[\"overflow_to_sample_mapping\"]\n",
    "    tokenized_inputs.pop(\"overflow_to_sample_mapping\", None)\n",
    "    \n",
    "    return tokenized_inputs, overflow_mapping, _word_ids\n",
    "\n",
    "def transform_image(image):\n",
    "    h = image.shape[0]\n",
    "    w = image.shape[1]\n",
    "    img_trans = TransformList([ResizeTransform(h=h, w=w, new_h=224, new_w=224)])\n",
    "    image = torch.tensor(img_trans.apply_image(image).copy()).permute(2, 0, 1)  # copy to make it writeable\n",
    "    return image, (w, h)\n",
    "\n",
    "def generate_example(img, page_df):\n",
    "    tokens = []\n",
    "    bboxes = []\n",
    "    norm_bboxes = []\n",
    "    img, size = transform_image(img)\n",
    "    for xtl, ytl, xbr, ybr, text, _ in page_df.itertuples(index=False):\n",
    "        box = [xtl, ytl, xbr, ybr]\n",
    "        tokens.append(text)\n",
    "        bboxes.append(box)\n",
    "        norm_bboxes.append(normalize_bbox(box, size))\n",
    "\n",
    "    return {\"tokens\": [tokens], \"bboxes\": [bboxes], \"norm_bboxes\": [norm_bboxes], \"image\": [img]}\n",
    "\n",
    "def convert_to_tensor(inputs):\n",
    "    inputs_t = dict()\n",
    "    for k, v in inputs.items():\n",
    "        if isinstance(v[0], list):\n",
    "            inputs_t[k] = torch.tensor(v)\n",
    "        elif isinstance(v[0], torch.Tensor):\n",
    "            inputs_t[k] = torch.stack(v)\n",
    "        else:\n",
    "            raise Exception(f\"{k} is a list of type {type(v[0])}\")\n",
    "    return inputs_t\n",
    "\n",
    "def get_class(text):\n",
    "    l = text.split(\"-\")\n",
    "    return l[1] if len(l) == 2 else \"OTHER\"\n",
    "\n",
    "def refine(words, boxes, tags, word_ids):\n",
    "    df = pd.DataFrame({\"word_id\": word_ids, \"tag\": tags})\n",
    "    df.dropna(inplace=True)\n",
    "    df = df.astype({\"word_id\": \"int32\"})\n",
    "    df = df.drop_duplicates([\"word_id\"])\n",
    "    df = df[df[\"tag\"] != \"O\"]\n",
    "    df[\"prefix\"] = df[\"tag\"].map(lambda x : x.split(\"-\")[0])\n",
    "    df[\"class\"] = df[\"tag\"].map(get_class)\n",
    "    df[\"word\"] = df[\"word_id\"].map(lambda x : words[x])\n",
    "    df[\"bbox\"] = df[\"word_id\"].map(lambda x : boxes[x])\n",
    "    return df\n",
    "\n",
    "def infer_one_page(page_data):\n",
    "    res = pd.DataFrame(columns=[\"xtl\", \"ytl\", \"xbr\", \"ybr\", \"text\", \"label\", \"page\"])\n",
    "    class_map = {\"SUPPLIER_NAME\": \"supplier_name\", \"SUPPLIER_ADDR\": \"supplier_addr\", \"TOTALAMOUNT\": \"totalAmount\"}\n",
    "\n",
    "    img = page_data[\"img\"]\n",
    "    page_df = page_data[\"anno\"]\n",
    "    page_id = np.unique(page_df[\"page\"]).item()\n",
    "\n",
    "    example = generate_example(img, page_df)\n",
    "    inputs, overflow_mapping, word_ids = tokenize_and_align_labels(example)\n",
    "    inputs_t = convert_to_tensor(inputs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs_t)\n",
    "        input_boxes = inputs_t[\"bbox\"].reshape([-1, 4]).tolist()\n",
    "        preds = torch.argmax(outputs.logits, -1).reshape([-1]).tolist()\n",
    "        \n",
    "    pred_tags = [class_labels[i] for i in preds]\n",
    "    words = example[\"tokens\"][0]\n",
    "    boxes = example[\"bboxes\"][0]\n",
    "    df = refine(words, boxes, pred_tags, word_ids)\n",
    "    # debug(img, df, os.path.join(debug_dir, \"commercial-invoice.jpg\"))\n",
    "    for _, tag, _, class_name, word, box in df.itertuples(index=False):\n",
    "        x1, y1, x2, y2 = map(round, box)\n",
    "        res = res.append({\"xtl\": x1, \"ytl\": y1, \"xbr\": x2, \"ybr\": y2, \"text\": word, \"label\": class_map[class_name], \"page\": page_id}, ignore_index=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.isdir(debug_dir):\n",
    "#     shutil.rmtree(debug_dir)\n",
    "# os.makedirs(debug_dir)\n",
    "\n",
    "def infer_one_doc(doc_data):\n",
    "    doc_df = pd.DataFrame(columns=[\"xtl\", \"ytl\", \"xbr\", \"ybr\", \"text\", \"label\", \"page\"])\n",
    "    for page_data in doc_data:\n",
    "        page_df = infer_one_page(page_data)\n",
    "        doc_df = doc_df.append(page_df, ignore_index=True)\n",
    "    return doc_df\n",
    "\n",
    "# input_dir = \"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/test\"\n",
    "input_dir = \"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/sdmgr_inference/20220425_FilteredData\"\n",
    "\n",
    "output_dir = \"20220525_layoutlmv2_outputs\"\n",
    "if os.path.isdir(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir)\n",
    "fnames = [\"9498101535.jpg\"]\n",
    "filenames = [x for x in os.listdir(input_dir) if os.path.splitext(x)[1] in [\".jpg\", \".png\", \".pdf\"]]\n",
    "\n",
    "for filename in filenames:\n",
    "    # if filename not in fnames: continue\n",
    "    prefix = os.path.splitext(filename)[0]\n",
    "    doc_data = textract_reader.get_data(filename)\n",
    "    if doc_data is None:\n",
    "        print(f\"Skip {filename} because Textract data is missing\")\n",
    "        continue\n",
    "    doc_df = infer_one_doc(doc_data)\n",
    "    # optional debug\n",
    "    doc_df.to_csv(os.path.join(output_dir, f\"{prefix}.csv\"), sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_adjacent(bi, bj):\n",
    "    if bi[\"label\"] != bj[\"label\"]:\n",
    "        return False\n",
    "    min_h = min(bi[\"ybr\"] - bi[\"ytl\"], bj[\"ybr\"] - bj[\"ytl\"])\n",
    "    min_w = min(bi[\"xbr\"] - bi[\"xtl\"], bj[\"xbr\"] - bj[\"xtl\"])\n",
    "    dx = min(bi[\"xbr\"], bj[\"xbr\"]) - max(bi[\"xtl\"], bj[\"xtl\"])\n",
    "    dy = min(bi[\"ybr\"], bj[\"ybr\"]) - max(bi[\"ytl\"], bj[\"ytl\"])\n",
    "\n",
    "    dxr = dx/min_w\n",
    "    dyr = dy/min_h\n",
    "\n",
    "    if bi[\"label\"] in [\"supplier_name\", \"supplier_addr\"] and dxr > 0.1 and dy > -min_h/2:\n",
    "        return True\n",
    "    if dyr > 0.2 and dx > -min_h:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def dfs(adj_mat, i, groups, gid):\n",
    "    if groups[i] > -1:\n",
    "        return groups\n",
    "    groups[i] = gid\n",
    "    for j in range(len(adj_mat[i])):\n",
    "        if adj_mat[i, j] == 1:\n",
    "            groups = dfs(adj_mat, j, groups, gid)\n",
    "    return groups\n",
    "\n",
    "def merge_words_one_page(page_df):\n",
    "    res = pd.DataFrame(columns=[\"text\", \"label\", \"xtl\", \"ytl\", \"xbr\", \"ybr\", \"page\"])\n",
    "    page_df = page_df.reset_index(drop=True)\n",
    "    adj_mat = np.zeros((len(page_df), len(page_df)), dtype=np.int32)\n",
    "    for i, bi in page_df.iterrows():\n",
    "        for j, bj in page_df.iterrows():\n",
    "            if is_adjacent(bi, bj):\n",
    "                adj_mat[i][j] = 1\n",
    "    \n",
    "    groups = [-1] * len(adj_mat)\n",
    "    gid = 0\n",
    "    for i in range(len(adj_mat)):\n",
    "        groups = dfs(adj_mat, i, groups, gid)\n",
    "        gid += 1\n",
    "    page_df[\"group\"] = groups\n",
    "    page_id = np.unique(page_df[\"page\"]).item()\n",
    "    for _, group_df in page_df.groupby(\"group\"):\n",
    "        label = np.unique(group_df[\"label\"]).item()\n",
    "        text = \" \".join(group_df[\"text\"].tolist())\n",
    "        xtl = group_df[\"xtl\"].min()\n",
    "        ytl = group_df[\"ytl\"].min()\n",
    "        xbr = group_df[\"xbr\"].max()\n",
    "        ybr = group_df[\"ybr\"].max()\n",
    "        res = res.append({\"text\": text, \"label\": label, \"xtl\": xtl, \"ytl\": ytl, \"xbr\": xbr, \"ybr\": ybr, \"page\": page_id}, ignore_index=True)\n",
    "    return res\n",
    "\n",
    "def is_aligned(b, x):\n",
    "    W = b[2] - b[0]\n",
    "    H = b[3] - b[1]\n",
    "    roi = [0, b[1]-50*H, b[2]+50*W, b[3]]\n",
    "    dx = min(roi[2], x[2]) - max(roi[0], x[0])\n",
    "    dx = max(dx, 0)\n",
    "    dy = min(roi[3], x[3]) - max(roi[1], x[1])\n",
    "    dy = max(dy, 0)\n",
    "    return dx*dy > 0\n",
    "\n",
    "class DataExtractor:\n",
    "\n",
    "    def __init__(self, country_currency_map_file, symbol_currency_map_file):\n",
    "        self.country_df = pd.read_csv(country_currency_map_file)\n",
    "        self.country_df[\"CountryCode\"] = self.country_df[\"CountryCode\"].astype(\"str\")\n",
    "        self.symbol_df = pd.read_csv(symbol_currency_map_file)\n",
    "\n",
    "    def __call__(self, doc_pred_df, doc_ocr_data):\n",
    "        res = {\"supplier_name\": \"\", \"totalAmount\": \"\", \"currencyCode\": \"\"}\n",
    "        # get total amount\n",
    "        total_amount, total_amount_full_text, total_amount_box, total_amount_page = self.extract_total_amount(doc_pred_df)\n",
    "        res[\"totalAmount\"] = total_amount\n",
    "        # get currency\n",
    "        currency = self.extract_currency(total_amount_full_text, total_amount_box, total_amount_page, doc_ocr_data, doc_pred_df)\n",
    "        res[\"currencyCode\"] = currency\n",
    "        \n",
    "        # get supplier name\n",
    "        supplier_name = \"\"\n",
    "        name_pool = doc_pred_df[(doc_pred_df[\"page\"] == total_amount_page) & (doc_pred_df[\"label\"] == \"supplier_name\")][\"text\"].tolist()\n",
    "        if name_pool:\n",
    "            supplier_name = sorted(name_pool, key=lambda x : len(x.split()), reverse=True)[0]\n",
    "        if not supplier_name:\n",
    "            for page, post_page_df in doc_pred_df.groupby(\"page\"):\n",
    "                name_pool = post_page_df[post_page_df[\"label\"] == \"supplier_name\"][\"text\"].tolist()\n",
    "                if name_pool:\n",
    "                    supplier_name = sorted(name_pool, key=lambda x : len(x.split()), reverse=True)[0]\n",
    "                    break\n",
    "        res[\"supplier_name\"] = supplier_name\n",
    "        return res\n",
    "\n",
    "    def extract_total_amount(self, doc_pred_df):\n",
    "        total_amount = \"\"\n",
    "        total_amount_full_text = \"\"\n",
    "        total_amount_box = None\n",
    "        total_amount_page = None\n",
    "        for text, _, xtl, ytl, xbr, ybr, page_id in doc_pred_df[doc_pred_df[\"label\"] == \"totalAmount\"].itertuples(index=False):\n",
    "            m = re.search(\"\\d+(?:.\\d|,\\d)*\\d*\", text)\n",
    "            if m is not None:\n",
    "                subtext = m.group()\n",
    "                # print(subtext)\n",
    "                subtext = re.sub(\",\", \".\", subtext)\n",
    "                subtext = re.sub(\"[.](?=.*[.]|\\d{3})\", \"\", subtext)\n",
    "                # print(subtext)\n",
    "                try:\n",
    "                    t = float(subtext)\n",
    "                except:\n",
    "                    t = None\n",
    "                else:\n",
    "                    total_amount = subtext\n",
    "                    total_amount_full_text = text\n",
    "                    total_amount_box = [xtl, ytl, xbr, ybr]\n",
    "                    total_amount_page = page_id\n",
    "                    break\n",
    "        return total_amount, total_amount_full_text, total_amount_box, total_amount_page\n",
    "\n",
    "    def extract_currency(self, total_amount_full_text, total_amount_box, total_amount_page, doc_ocr_data, doc_pred_df):\n",
    "        symbol = \"\"\n",
    "        currency_code = \"\"\n",
    "        # extract currency from total amount text\n",
    "        if total_amount_full_text:\n",
    "            for sym, code in self.symbol_df.itertuples(index=False):\n",
    "                if sym in total_amount_full_text:\n",
    "                    currency_code = code\n",
    "                    symbol = sym\n",
    "                    if symbol != \"$\":\n",
    "                        break\n",
    "        # print(symbol, currency_code)\n",
    "        if not currency_code:\n",
    "            # extract currency from neighbor boxes -> full page\n",
    "            \n",
    "            aligned_list=[]\n",
    "            non_aligned_list=[]\n",
    "            for page_data in doc_ocr_data:\n",
    "                page_df = page_data[\"anno\"]\n",
    "                page_id = np.unique(page_df[\"page\"]).item()\n",
    "                if page_id != total_amount_page:\n",
    "                    continue\n",
    "                # print(total_amount_box)\n",
    "                for xtl, ytl, xbr, ybr, text, page in page_df.itertuples(index=False):\n",
    "                    box = [xtl, ytl, xbr, ybr]\n",
    "                    for sym, code in self.symbol_df.itertuples(index=False):\n",
    "                        if sym in text:\n",
    "                            if total_amount_box is not None and is_aligned(total_amount_box, box):\n",
    "                                aligned_list.append((sym, code))\n",
    "                            else:\n",
    "                                non_aligned_list.append((sym, code))\n",
    "            \n",
    "            if not aligned_list and not non_aligned_list:\n",
    "                for page_data in doc_ocr_data:\n",
    "                    page_df = page_data[\"anno\"]\n",
    "                    for xtl, ytl, xbr, ybr, text, page in page_df.itertuples(index=False):\n",
    "                        box = [xtl, ytl, xbr, ybr]\n",
    "                        for sym, code in self.symbol_df.itertuples(index=False):\n",
    "                            if sym in text:\n",
    "                                if total_amount_box is not None and is_aligned(total_amount_box, box):\n",
    "                                    aligned_list.append((sym, code))\n",
    "                                else:\n",
    "                                    non_aligned_list.append((sym, code))\n",
    "            \n",
    "            aligned_list = Counter(aligned_list)\n",
    "            non_aligned_list = Counter(non_aligned_list)\n",
    "            if aligned_list:\n",
    "                symbol, currency_code = aligned_list.most_common()[0][0]\n",
    "                if symbol == \"$\" and len(aligned_list) > 1:\n",
    "                    s, c = aligned_list.most_common()[1][0]\n",
    "                    if c == currency_code:\n",
    "                        symbol = s\n",
    "            elif non_aligned_list:\n",
    "                symbol, currency_code = non_aligned_list.most_common()[0][0]\n",
    "                if symbol == \"$\" and len(non_aligned_list) > 1:\n",
    "                    s, c = non_aligned_list.most_common()[1][0]\n",
    "                    if c == currency_code:\n",
    "                        symbol = s\n",
    "        # print(symbol, currency_code)\n",
    "        # extract currency by  address\n",
    "        if symbol in [\"\", \"$\"]:\n",
    "            codes = []\n",
    "            addresses = doc_pred_df[(doc_pred_df[\"label\"] == \"supplier_addr\") & (doc_pred_df[\"page\"] == total_amount_page)][\"text\"]\n",
    "            for address in addresses:\n",
    "                for country, country_code ,_, code in self.country_df.itertuples(index=False):\n",
    "                    if country.lower() in address.lower():\n",
    "                        # print(country)\n",
    "                        codes.append(code)\n",
    "            codes = Counter(codes)\n",
    "            if codes:\n",
    "                code = codes.most_common()[0][0]\n",
    "                if symbol == \"\":\n",
    "                    currency_code = code\n",
    "                elif symbol == \"$\":\n",
    "                    if code in [\"USD\", \"SGD\", \"HKD\", \"NZD\", \"AUD\", \"CAD\", \"TWD\", \"HKD\"]:\n",
    "                        currency_code = code\n",
    "        # print(symbol, currency_code)\n",
    "        return currency_code\n",
    "        \n",
    "extractor = DataExtractor(\"../../../sdmgr_inference/country_currency_mapping.csv\", \"../../../sdmgr_inference/currency_symbol_mapping.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"20220525_layoutlmv2_results.csv\"\n",
    "out_df = pd.DataFrame(columns=[\"code\", \"supplier_name\", \"totalAmount\", \"currencyCode\"])\n",
    "# fnames = [\"9506922289\"]\n",
    "filenames = os.listdir(\"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/sdmgr_inference/20220425_FilteredData\")\n",
    "from glob import glob\n",
    "import requests\n",
    "csv_paths = glob(os.path.join(\"20220525_layoutlmv2_outputs\", \"*csv\"))\n",
    "for csv_path in csv_paths:\n",
    "    prefix = os.path.splitext(os.path.basename(csv_path))[0]\n",
    "    # if prefix not in fnames: continue\n",
    "    print(csv_path)\n",
    "    for name in filenames:\n",
    "        if prefix in name:\n",
    "            filename = name\n",
    "    doc_ocr_data = textract_reader.get_data(filename, return_img=False)\n",
    "    doc_kie_df = pd.read_csv(csv_path)\n",
    "    doc_kie_df = doc_kie_df.astype({\"text\": \"str\"})\n",
    "    doc_pred_df = pd.DataFrame(columns=[\"text\", \"label\", \"xtl\", \"ytl\", \"xbr\", \"ybr\", \"page\"])\n",
    "    for page, page_kie_df in doc_kie_df.groupby(\"page\"):\n",
    "        page_pred_df = merge_words_one_page(page_kie_df)\n",
    "        doc_pred_df = doc_pred_df.append(page_pred_df, ignore_index=True)\n",
    "    # print(doc_pred_df)\n",
    "    doc_out = {\"code\": prefix}\n",
    "    ext_out = extractor(doc_pred_df, doc_ocr_data)\n",
    "    doc_out.update(ext_out)\n",
    "    out_df = out_df.append(doc_out, ignore_index=True)\n",
    "\n",
    "out_df.to_csv(result_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_df = pd.read_csv(\"20220525_layoutlmv2_results.csv\")\n",
    "prd_df.columns = [\"code\", \"pred_supplier_name\", \"pred_totalAmount\", \"pred_currencyCode\"]\n",
    "gt_df = pd.read_csv(\"/media/minh/Storage/projects/EGS/InvoiceDataExtraction/metadata.csv\")\n",
    "gt_df[\"true_supplier_name,true_totalAmount,true_currencyCode\".split(\",\")] = gt_df[\"supplier_name,totalAmount,currencyCode\".split(\",\")]\n",
    "gt_df = gt_df[\"code,true_supplier_name,true_totalAmount,true_currencyCode\".split(\",\")]\n",
    "df = prd_df.merge(gt_df, on=\"code\", how=\"inner\")\n",
    "df[\"correct_supplier_name\"] = \"\"\n",
    "df[\"correct_totalAmount\"] = \"\"\n",
    "df[\"correct_currencyCode\"] = \"\"\n",
    "df = df[[\"code\", \"pred_supplier_name\", \"true_supplier_name\", \"correct_supplier_name\", \"pred_totalAmount\", \"true_totalAmount\", \"pred_currencyCode\", \"true_currencyCode\", \"correct_currencyCode\"]]\n",
    "\n",
    "# df.to_excel(\"20220525_layoutlmv2_results_eval.xlsx\", index=False)\n",
    "df.to_csv(\"20220525_layoutlmv2_results_eval.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"20220525_layoutlmv2_results_eval.csv\")\n",
    "print(np.sum(d[\"pred_currencyCode\"] != d[\"true_currencyCode\"]), np.sum(d[\"pred_totalAmount\"] != d[\"true_totalAmount\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "942140736990c62f967dadc2617fa6d17aee4a6bcb2f21bd66eacc795d470755"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
